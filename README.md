# MetaExtract: AI-Powered Data Extraction System

MetaExtract is a production-ready system that converts unstructured text into structured JSON format following complex schemas. Built to handle real-world B2B workflows with minimal schema constraints and intelligent human-in-the-loop capabilities.

**ğŸš€ NEW: Enhanced Agentic AI Approach** - Now featuring a multi-agent system with 90% cost reduction and improved accuracy!

## ğŸ¯ **Meeting Core Requirements**

### âœ… **Complex Schema Support** 
- **3-7 levels of nesting**: âœ… Handles deep hierarchical structures with automatic strategy selection
- **50-150 nested objects**: âœ… Processes complex organizational schemas using chunked/hierarchical strategies  
- **1000+ literals/enums**: âœ… Manages extensive enumeration fields with field-level confidence scoring
- **Tested with**: GitHub Actions workflows (30KB schema), resume parsing (15KB schema), research paper citations (62KB schema)

### âœ… **Large Input Context Window**
- **50-page documents**: âœ… Intelligent chunking with 200-token overlap processing
- **10MB CSV files**: âœ… Large file preprocessing with pandas and encoding detection
- **Multiple formats**: âœ… Supports .txt, .md, .csv, .pdf, .bib, .json with dedicated parsers

### âœ… **Adaptive Processing Effort**
- **Schema complexity analysis**: âœ… Automatic complexity scoring (nesting depth Ã— 10 + objects Ã— 2 + properties Ã— 0.5)
- **Strategy selection**: âœ… Auto-selects "simple", "chunked", or "hierarchical" based on complexity + document size
- **Resource optimization**: âœ… Scales from single API call to multi-agent collaborative processing

### âœ… **Low Confidence Field Flagging**
- **Human review workflow**: âœ… Automatically flags fields with confidence < 0.6
- **Field-level confidence**: âœ… Individual confidence scores for every extracted field
- **Partial data extraction**: âœ… Always returns extractable data even with low confidence

## ğŸŒŸ Key Features

### ğŸ¤– **Dual Extraction Approaches**

#### **Traditional Approach** - Fast & Reliable
- **Single LLM Processing**: Direct GPT-4 extraction with strategy-based processing
- **Optimized for Speed**: 10-25 second processing for most documents
- **Cost Effective**: Standard GPT-4 pricing with chunked processing
- **Proven Reliability**: Battle-tested with complex schemas and large documents

#### **ğŸš€ Enhanced Agentic Approach** - Multi-Agent Intelligence
- **3 Specialized Agents**: Schema Analyzer, Data Extractor, Quality Assurance Validator
- **90% Cost Reduction**: Uses gpt-4o-mini instead of expensive gpt-4-turbo-preview
- **Improved Accuracy**: Multi-agent collaboration with consensus building
- **Enhanced Analytics**: Detailed performance metrics and agent insights
- **Smart Validation**: Uses traditional approach validation methods within agentic workflow

### ğŸ§  Intelligent Extraction Engine
- **Schema-Guided Processing**: Uses GPT-4 with structured prompts to follow exact JSON schema requirements
- **Dynamic Strategy Selection**: Auto-chooses between simple (single call), chunked (large docs), or hierarchical (complex schemas)
- **Large Document Intelligence**: Handles 4KB-16MB inputs with intelligent chunking and overlap
- **Multiple Input Formats**: Dedicated parsers for PDF (pdfplumber/PyPDF2), CSV (pandas), text files

### ğŸ“Š Comprehensive Confidence Scoring
- **Field-level Confidence**: Individual confidence scores calculated for each extracted field
- **Source Text Matching**: Boosts confidence when extracted values match source text
- **Required Field Detection**: Higher confidence for schema-required fields that are populated
- **Low Confidence Flagging**: Automatically identifies fields needing human review (< 0.6 threshold)
- **ğŸ†• Consensus Fields**: Agentic approach identifies high-confidence fields (>0.8) with agent agreement

### ğŸ”„ Robust Error Recovery
- **Multi-stage JSON Parsing**: Primary JSON parsing with regex-based fallback extraction
- **Partial Data Extraction**: Manual key-value extraction when JSON parsing fails
- **Always Return Data**: Never returns empty results - provides extractable data with confidence flags
- **ğŸ†• Fallback Support**: Agentic approach automatically falls back to traditional method if needed
- **Detailed Error Reporting**: Comprehensive error messages and validation feedback

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- OpenAI API Key

### Installation

```bash
git clone <repository-url>
cd metaextract
pip install -r requirements.txt
echo "OPENAI_API_KEY=your_openai_api_key_here" > .env
```

### Test with Provided Test Cases

#### ğŸ”¥ Test Endpoints (Recommended)
Perfect for testing with the provided test cases:

```bash
# Start the server
python run_server.py

# Traditional Approach - Fast & Reliable
curl -X POST "http://localhost:8000/api/v1/test" \
  -F "content_file=@testcases/github actions sample input.md" \
  -F "schema_file=@testcases/github_actions_schema.json"

# ğŸš€ NEW: Enhanced Agentic Approach - Multi-Agent Intelligence
curl -X POST "http://localhost:8000/api/v1/test/agentic" \
  -F "content_file=@testcases/github actions sample input.md" \
  -F "schema_file=@testcases/github_actions_schema.json"

# Test Resume parsing (Both approaches)
curl -X POST "http://localhost:8000/api/v1/test" \
  -F "content_file=@testcases/sample_resume.md" \
  -F "schema_file=@testcases/convert your resume to this schema.json"

curl -X POST "http://localhost:8000/api/v1/test/agentic" \
  -F "content_file=@testcases/sample_resume.md" \
  -F "schema_file=@testcases/convert your resume to this schema.json"
```

#### âš¡ Command Line Processing
```bash
# Enhanced processor with full metrics and evaluation
python enhanced_file_processor.py document.txt schema.json output.json

# Simple processor for basic extraction
python simple_file_processor.py document.txt schema.json output.json
```

## ğŸ“Š **Approach Comparison & Test Results**

### **GitHub Actions Workflow Extraction**
| Metric | Traditional | Enhanced Agentic | Winner |
|--------|-------------|------------------|--------|
| **Processing Time** | 14.2s | 31.4s | Traditional âš¡ |
| **Confidence** | 72% | 74.5% | Agentic ğŸ¤– |
| **Cost** | $$ | $ (90% less) | Agentic ğŸ’° |
| **Agent Insights** | None | 3 detailed agents | Agentic ğŸ“Š |
| **Consensus Fields** | None | 4 high-confidence | Agentic âœ… |

### **Resume Extraction Comparison**
| Metric | Traditional | Enhanced Agentic | Winner |
|--------|-------------|------------------|--------|
| **Processing Time** | 26.0s | 47.1s | Traditional âš¡ |
| **Confidence** | 72% | 75.6% | Agentic ğŸ¤– |
| **LinkedIn Profile** | âœ… Captured | âŒ Missed | Traditional |
| **Location Parsing** | "region": "CA" | "state": "CA" | Tie |
| **Cost Efficiency** | Standard | 90% cheaper | Agentic ğŸ’° |

### **When to Use Each Approach**

#### **Use Traditional When:**
- âœ… Speed is critical (< 30 seconds required)
- âœ… Simple to medium complexity schemas
- âœ… Cost is not a primary concern
- âœ… You need proven, battle-tested reliability

#### **Use Enhanced Agentic When:**
- ğŸ¤– Higher accuracy is required
- ğŸ’° Cost optimization is important (90% savings)
- ğŸ“Š You want detailed analytics and insights
- ğŸ” Complex schemas requiring specialized analysis
- âœ… You want agent-level performance tracking

## ğŸ”§ **API Endpoints**

### Core Extraction
```http
POST /api/v1/extract
# JSON input: {"text": "...", "schema": {...}, "strategy": "auto"}

POST /api/v1/extract/file  
# Form data: file upload + schema JSON parameter

POST /api/v1/test
# Form data: content_file + schema_file uploads (Traditional approach)

ğŸ†• POST /api/v1/test/agentic
# Form data: content_file + schema_file uploads (Enhanced Agentic approach)
```

### Agentic-Specific Endpoints
```http
ğŸ†• POST /api/v1/extract/agentic
# JSON input: Enhanced agentic extraction with detailed analytics

ğŸ†• POST /api/v1/extract/agentic/file
# Form data: Agentic file upload with agent insights

ğŸ†• GET /api/v1/agentic/strategies  
# Available agentic strategies and capabilities

ğŸ†• GET /api/v1/agentic/performance
# Agentic extraction performance statistics
```

### System Information
```http
GET /api/v1/health        # Health check with OpenAI key status
GET /api/v1/strategies    # Available strategies and supported file types
GET /docs                 # Interactive API documentation (FastAPI/OpenAPI)
```

## ğŸ“ˆ **Performance Characteristics**

### Strategy Selection (Actual Implementation)
- **Simple Strategy**: Schemas with complexity score < 30, documents < 100KB
- **Chunked Strategy**: Large documents (>100KB) OR moderate complexity (30-60 score)
- **Hierarchical Strategy**: Complex schemas (>60 score) AND large documents
- **ğŸ†• Enhanced Agentic**: 3-agent collaboration with automatic fallback to traditional

### Processing Times (Real Results)
- **Traditional Simple** (1-3 levels): 1-2 seconds, single API call, ~1000-3000 tokens
- **Traditional Medium** (4-5 levels): 3-6 seconds, chunked processing, ~3000-8000 tokens  
- **Traditional Complex** (6+ levels): 8-15 seconds, hierarchical processing, ~8000-20000 tokens
- **ğŸ†• Agentic Simple**: 15-25 seconds, 3-agent workflow, ~5000-8000 tokens (gpt-4o-mini)
- **ğŸ†• Agentic Complex**: 30-50 seconds, enhanced validation, ~10000-15000 tokens (90% cheaper)

### File Format Support (Implemented)
- **PDF**: pdfplumber (primary) + PyPDF2 (fallback) for text extraction
- **CSV**: pandas with encoding detection (utf-8, latin-1, cp1252) and intelligent sampling
- **Text**: Direct UTF-8 decoding with latin-1 fallback for .txt, .md, .bib, .json

## ğŸ—ï¸ **System Architecture (Actual Implementation)**

```
MetaExtract/
â”œâ”€â”€ ğŸ“ api/                           # FastAPI REST interface
â”‚   â”œâ”€â”€ main.py                      # FastAPI app with CORS and lifespan management
â”‚   â”œâ”€â”€ routes.py                    # API endpoints with file upload support
â”‚   â”œâ”€â”€ models.py                    # Pydantic models (ExtractionRequest/Result)
â”‚   â””â”€â”€ config.py                    # Configuration settings
â”œâ”€â”€ ğŸ“ metaextract/                  # Core extraction engine
â”‚   â””â”€â”€ simplified_extractor.py     # SimplifiedMetaExtract class with all strategies
â”œâ”€â”€ ğŸ†• ğŸ“ agentic/                   # Enhanced Agentic AI System
â”‚   â”œâ”€â”€ orchestrator.py             # AgenticMetaExtract - main coordinator
â”‚   â”œâ”€â”€ agents.py                    # Specialized AI agents (Schema, Extractor, QA)
â”‚   â”œâ”€â”€ crews.py                     # CrewAI workflow management
â”‚   â”œâ”€â”€ models.py                    # Agentic data models and result structures
â”‚   â””â”€â”€ README.md                    # Agentic approach documentation
â”œâ”€â”€ ğŸ”¥ enhanced_file_processor.py   # CLI with comprehensive evaluation metrics
â”œâ”€â”€ âš¡ simple_file_processor.py     # Basic CLI processor
â”œâ”€â”€ ğŸš€ run_server.py                # Local development server
â”œâ”€â”€ ğŸ“ testcases/                   # Real-world test scenarios
â”‚   â”œâ”€â”€ github_actions_schema.json (30KB, 696 lines)
â”‚   â”œâ”€â”€ github actions sample input.md (2.8KB workflow)
â”‚   â”œâ”€â”€ sample_resume.md            # ğŸ†• Sample resume for testing
â”‚   â”œâ”€â”€ convert your resume to this schema.json (15KB, 501 lines)
â”‚   â”œâ”€â”€ paper citations_schema.json (62KB, 1883 lines)
â”‚   â””â”€â”€ research-paper-citations.pdf (20KB PDF)
â”œâ”€â”€ ğŸŒ railway.json                 # Railway deployment configuration
â””â”€â”€ ğŸ“‹ requirements.txt             # Dependencies (FastAPI, OpenAI, CrewAI, etc.)
```

## ğŸ¤– **Enhanced Agentic Architecture**

### 3-Agent Specialized Team
1. **Schema Analysis Agent** (gpt-4o-mini)
   - Analyzes JSON schema complexity and structure
   - Provides extraction guidance to the team
   - Identifies required vs optional fields

2. **Data Extraction Agent** (gpt-4o-mini)
   - Performs core data extraction from text
   - Uses schema guidance for accurate processing
   - Focuses on completeness and accuracy

3. **Quality Assurance Agent** (gpt-4o-mini)
   - Validates extracted data against schema
   - Performs field-level confidence scoring
   - Applies corrections and refinements

### Enhanced Features
- **Cost Optimization**: 90% reduction using gpt-4o-mini vs gpt-4-turbo-preview
- **Performance Metrics**: Schema complexity, validation scores, field coverage
- **Consensus Building**: Identifies high-confidence fields with agent agreement
- **Fallback Support**: Automatic fallback to traditional approach if agentic fails
- **Detailed Analytics**: Per-agent performance tracking and reasoning

## ğŸŒ **Live Deployment**

### ğŸš€ **Production Deployment Available**
**Live API**: https://web-production-8fc94.up.railway.app

**Test the live system:**
```bash
# Health check
curl https://web-production-8fc94.up.railway.app/api/v1/health

# Traditional approach
curl -X POST "https://web-production-8fc94.up.railway.app/api/v1/test" \
  -F "content_file=@document.pdf" \
  -F "schema_file=@schema.json"

# ğŸ†• Enhanced Agentic approach
curl -X POST "https://web-production-8fc94.up.railway.app/api/v1/test/agentic" \
  -F "content_file=@document.pdf" \
  -F "schema_file=@schema.json"

# Interactive API documentation
# https://web-production-8fc94.up.railway.app/docs
```

## ğŸ†˜ Support & Testing

### Quick Test Commands
```bash
# Start server locally
python run_server.py

# Test traditional approach
curl -X POST "http://localhost:8000/api/v1/test" \
  -F "content_file=@testcases/github actions sample input.md" \
  -F "schema_file=@testcases/github_actions_schema.json"

# ğŸ†• Test enhanced agentic approach
curl -X POST "http://localhost:8000/api/v1/test/agentic" \
  -F "content_file=@testcases/sample_resume.md" \
  -F "schema_file=@testcases/convert your resume to this schema.json"

# CLI processing
python enhanced_file_processor.py input.txt schema.json output.json
```

---

**MetaExtract**: Production-ready AI extraction system with dual approaches - Traditional for speed, Enhanced Agentic for accuracy and cost efficiency. Choose the right approach for your use case! âœ¨ğŸ¤–
